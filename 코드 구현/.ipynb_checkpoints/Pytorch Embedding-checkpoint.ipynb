{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20121a4f-4e30-4145-bd73-e37fd7f75b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from .module import Module\n",
    "from .. import functional as F\n",
    "from .. import init\n",
    "\n",
    "__all__ = ['Embedding', 'EmbeddingBag']\n",
    "\n",
    "[docs]class Embedding(Module):\n",
    "    r\"\"\"A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "\n",
    "    This module is often used to store word embeddings and retrieve them using indices.\n",
    "    The input to the module is a list of indices, and the output is the corresponding\n",
    "    word embeddings.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): size of the dictionary of embeddings\n",
    "        embedding_dim (int): the size of each embedding vector\n",
    "        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n",
    "                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n",
    "                                     i.e. it remains as a fixed \"pad\". For a newly constructed Embedding,\n",
    "                                     the embedding vector at :attr:`padding_idx` will default to all zeros,\n",
    "                                     but can be updated to another value to be used as the padding vector.\n",
    "        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n",
    "                                    is renormalized to have norm :attr:`max_norm`.\n",
    "        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n",
    "        scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n",
    "                                                the words in the mini-batch. Default ``False``.\n",
    "        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n",
    "                                 See Notes for more details regarding sparse gradients.\n",
    "\n",
    "    Attributes:\n",
    "        weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n",
    "                         initialized from :math:`\\mathcal{N}(0, 1)`\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
    "        - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n",
    "\n",
    "    .. note::\n",
    "        Keep in mind that only a limited number of optimizers support\n",
    "        sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n",
    "        :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n",
    "\n",
    "    .. note::\n",
    "        When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the\n",
    "        :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be\n",
    "        modified in-place, performing a differentiable operation on ``Embedding.weight`` before\n",
    "        calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when\n",
    "        :attr:`max_norm` is not ``None``. For example::\n",
    "\n",
    "            n, d, m = 3, 5, 7\n",
    "            embedding = nn.Embedding(n, d, max_norm=True)\n",
    "            W = torch.randn((m, d), requires_grad=True)\n",
    "            idx = torch.tensor([1, 2])\n",
    "            a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
    "            b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "            out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "            loss = out.sigmoid().prod()\n",
    "            loss.backward()\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> # an Embedding module containing 10 tensors of size 3\n",
    "        >>> embedding = nn.Embedding(10, 3)\n",
    "        >>> # a batch of 2 samples of 4 indices each\n",
    "        >>> input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
    "        >>> embedding(input)\n",
    "        tensor([[[-0.0251, -1.6902,  0.7172],\n",
    "                 [-0.6431,  0.0748,  0.6969],\n",
    "                 [ 1.4970,  1.3448, -0.9685],\n",
    "                 [-0.3677, -2.7265, -0.1685]],\n",
    "\n",
    "                [[ 1.4970,  1.3448, -0.9685],\n",
    "                 [ 0.4362, -0.4004,  0.9400],\n",
    "                 [-0.6431,  0.0748,  0.6969],\n",
    "                 [ 0.9124, -2.3616,  1.1151]]])\n",
    "\n",
    "\n",
    "        >>> # example with padding_idx\n",
    "        >>> embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "        >>> input = torch.LongTensor([[0, 2, 0, 5]])\n",
    "        >>> embedding(input)\n",
    "        tensor([[[ 0.0000,  0.0000,  0.0000],\n",
    "                 [ 0.1535, -2.0309,  0.9315],\n",
    "                 [ 0.0000,  0.0000,  0.0000],\n",
    "                 [-0.1655,  0.9897,  0.0635]]])\n",
    "\n",
    "        >>> # example of changing `pad` vector\n",
    "        >>> padding_idx = 0\n",
    "        >>> embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n",
    "        >>> embedding.weight\n",
    "        Parameter containing:\n",
    "        tensor([[ 0.0000,  0.0000,  0.0000],\n",
    "                [-0.7895, -0.7089, -0.0364],\n",
    "                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n",
    "        >>> with torch.no_grad():\n",
    "        ...     embedding.weight[padding_idx] = torch.ones(3)\n",
    "        >>> embedding.weight\n",
    "        Parameter containing:\n",
    "        tensor([[ 1.0000,  1.0000,  1.0000],\n",
    "                [-0.7895, -0.7089, -0.0364],\n",
    "                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',\n",
    "                     'norm_type', 'scale_grad_by_freq', 'sparse']\n",
    "\n",
    "    num_embeddings: int\n",
    "    embedding_dim: int\n",
    "    padding_idx: Optional[int]\n",
    "    max_norm: Optional[float]\n",
    "    norm_type: float\n",
    "    scale_grad_by_freq: bool\n",
    "    weight: Tensor\n",
    "    freeze: bool\n",
    "    sparse: bool\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None,\n",
    "                 max_norm: Optional[float] = None, norm_type: float = 2., scale_grad_by_freq: bool = False,\n",
    "                 sparse: bool = False, _weight: Optional[Tensor] = None, _freeze: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        if padding_idx is not None:\n",
    "            if padding_idx > 0:\n",
    "                assert padding_idx < self.num_embeddings, 'Padding_idx must be within num_embeddings'\n",
    "            elif padding_idx < 0:\n",
    "                assert padding_idx >= -self.num_embeddings, 'Padding_idx must be within num_embeddings'\n",
    "                padding_idx = self.num_embeddings + padding_idx\n",
    "        self.padding_idx = padding_idx\n",
    "        self.max_norm = max_norm\n",
    "        self.norm_type = norm_type\n",
    "        self.scale_grad_by_freq = scale_grad_by_freq\n",
    "        if _weight is None:\n",
    "            self.weight = Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n",
    "                                    requires_grad=not _freeze)\n",
    "            self.reset_parameters()\n",
    "        else:\n",
    "            assert list(_weight.shape) == [num_embeddings, embedding_dim], \\\n",
    "                'Shape of weight does not match num_embeddings and embedding_dim'\n",
    "            self.weight = Parameter(_weight, requires_grad=not _freeze)\n",
    "\n",
    "        self.sparse = sparse\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        init.normal_(self.weight)\n",
    "        self._fill_padding_idx_with_zero()\n",
    "\n",
    "    def _fill_padding_idx_with_zero(self) -> None:\n",
    "        if self.padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                self.weight[self.padding_idx].fill_(0)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.embedding(\n",
    "            input, self.weight, self.padding_idx, self.max_norm,\n",
    "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        s = '{num_embeddings}, {embedding_dim}'\n",
    "        if self.padding_idx is not None:\n",
    "            s += ', padding_idx={padding_idx}'\n",
    "        if self.max_norm is not None:\n",
    "            s += ', max_norm={max_norm}'\n",
    "        if self.norm_type != 2:\n",
    "            s += ', norm_type={norm_type}'\n",
    "        if self.scale_grad_by_freq is not False:\n",
    "            s += ', scale_grad_by_freq={scale_grad_by_freq}'\n",
    "        if self.sparse is not False:\n",
    "            s += ', sparse=True'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "[docs]    @classmethod\n",
    "    def from_pretrained(cls, embeddings, freeze=True, padding_idx=None,\n",
    "                        max_norm=None, norm_type=2., scale_grad_by_freq=False,\n",
    "                        sparse=False):\n",
    "        r\"\"\"Create Embedding instance from given 2-dimensional FloatTensor.\n",
    "\n",
    "        Args:\n",
    "            embeddings (Tensor): FloatTensor containing weights for the Embedding.\n",
    "                First dimension is being passed to Embedding as ``num_embeddings``, second as ``embedding_dim``.\n",
    "            freeze (bool, optional): If ``True``, the tensor does not get updated in the learning process.\n",
    "                Equivalent to ``embedding.weight.requires_grad = False``. Default: ``True``\n",
    "            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n",
    "                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n",
    "                                         i.e. it remains as a fixed \"pad\".\n",
    "            max_norm (float, optional): See module initialization documentation.\n",
    "            norm_type (float, optional): See module initialization documentation. Default ``2``.\n",
    "            scale_grad_by_freq (bool, optional): See module initialization documentation. Default ``False``.\n",
    "            sparse (bool, optional): See module initialization documentation.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> # FloatTensor containing pretrained weights\n",
    "            >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "            >>> embedding = nn.Embedding.from_pretrained(weight)\n",
    "            >>> # Get embeddings for index 1\n",
    "            >>> input = torch.LongTensor([1])\n",
    "            >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
    "            >>> embedding(input)\n",
    "            tensor([[ 4.0000,  5.1000,  6.3000]])\n",
    "        \"\"\"\n",
    "        assert embeddings.dim() == 2, \\\n",
    "            'Embeddings parameter is expected to be 2-dimensional'\n",
    "        rows, cols = embeddings.shape\n",
    "        embedding = cls(\n",
    "            num_embeddings=rows,\n",
    "            embedding_dim=cols,\n",
    "            _weight=embeddings,\n",
    "            _freeze=freeze,\n",
    "            padding_idx=padding_idx,\n",
    "            max_norm=max_norm,\n",
    "            norm_type=norm_type,\n",
    "            scale_grad_by_freq=scale_grad_by_freq,\n",
    "            sparse=sparse)\n",
    "        return embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
