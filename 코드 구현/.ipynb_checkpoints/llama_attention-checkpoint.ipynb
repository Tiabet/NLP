{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ccbabd-beda-4fe1-8893-c310cd336eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = config['n_heads']\n",
    "        self.n_kv_heads = config['n_kv_heads']\n",
    "        self.dim = config['embed_dim']\n",
    "        self.n_kv_heads = self.n_heads if self.n_kv_heads is None else self.n_kv_heads\n",
    "        self.n_heads_q = self.n_heads\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(self.n_heads * self.head_dim, self.dim, bias=False)\n",
    "\n",
    "        self.cache = KVCache(\n",
    "            max_batch_size=config['max_batch_size'],\n",
    "            max_seq_len=config['max_seq_len'],\n",
    "            n_kv_heads=self.n_kv_heads,\n",
    "            head_dim=self.head_dim,\n",
    "            device=config['device']\n",
    "        )\n",
    "\n",
    "    def forward(self, x, start_pos, freqs_complex):\n",
    "\n",
    "        # seq_len is always 1 during inference\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # (m, seq_len, dim)\n",
    "        xq = self.wq(x)\n",
    "\n",
    "        # (m, seq_len, h_kv * head_dim)\n",
    "        xk = self.wk(x)\n",
    "        xv = self.wv(x)\n",
    "\n",
    "        # (m, seq_len, n_heads, head_dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "\n",
    "        # (m, seq_len, h_kv, head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # (m, seq_len, num_head, head_dim)\n",
    "        xq = apply_rotary_embeddings(xq, freqs_complex, device=x.device)\n",
    "\n",
    "        # (m, seq_len, h_kv, head_dim)\n",
    "        xk = apply_rotary_embeddings(xk, freqs_complex, device=x.device)\n",
    "\n",
    "        # replace the entry in the cache\n",
    "        self.cache.update(batch_size, start_pos, xk, xv)\n",
    "\n",
    "        # (m, seq_len, h_kv, head_dim)\n",
    "        keys, values = self.cache.get(batch_size, start_pos, seq_len)\n",
    "\n",
    "        # (m, seq_len, h_kv, head_dim) --> (m, seq_len, n_heads, head_dim)\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "\n",
    "        # (m, n_heads, seq_len, head_dim)\n",
    "        # seq_len is 1 for xq during inference\n",
    "        xq = xq.transpose(1, 2)\n",
    "\n",
    "        # (m, n_heads, seq_len, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # (m, n_heads, seq_len_q, head_dim) @ (m, n_heads, head_dim, seq_len) -> (m, n_heads, seq_len_q, seq_len)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # (m, n_heads, seq_len_q, seq_len)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "\n",
    "        # (m, n_heads, seq_len_q, seq_len) @ (m, n_heads, seq_len, head_dim) -> (m, n_heads, seq_len_q, head_dim)\n",
    "        output = torch.matmul(scores, values)\n",
    "\n",
    "        # ((m, n_heads, seq_len_q, head_dim) -> (m, seq_len_q, dim)\n",
    "        output = (output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
    "\n",
    "        # (m, seq_len_q, dim)\n",
    "        return self.wo(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
